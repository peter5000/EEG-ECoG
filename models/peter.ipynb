{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../EEG-ECoG') # adding path for packages\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import scipy.io\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from utils.dataloader import dataloader\n",
    "from sklearn.decomposition import PCA\n",
    "from utils import data_preprocessing as dp\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple linear regression model\n",
    "class LinearRegressionModel(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(LinearRegressionModel, self).__init__()\n",
    "        self.linear = nn.Linear(input_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19, 323262)\n",
      "(129, 319234)\n"
     ]
    }
   ],
   "source": [
    "# Import data\n",
    "eeg_path = '../../Datasets/20110607S2_EEGECoG_Su_Oosugi_ECoG128-EEG18/20110607S2_EEGECoG_Su_Oosugi_ECoG128-EEG18_mat/EEG05_anesthesia.mat'\n",
    "ecog_path = '../../Datasets/20110607S2_EEGECoG_Su_Oosugi_ECoG128-EEG18/20110607S2_EEGECoG_Su_Oosugi_ECoG128-EEG18_mat/ECoG05_anesthesia.mat'\n",
    "_, eeg_data = dp.loadMatFile(eeg_path)\n",
    "_, ecog_data = dp.loadMatFile(ecog_path)\n",
    "print(eeg_data.shape)  # (19, 323262)\n",
    "print(ecog_data.shape) # (129, 319234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(323262,)\n",
      "(19, 323262)\n",
      "eeg_data.shape:  (319234, 19)\n",
      "ecog_data.shape:  (319234, 129)\n"
     ]
    }
   ],
   "source": [
    "# downsample eeg to ecog\n",
    "# for ch in range(eeg_data.shape[0]):\n",
    "#     new_eeg_data = np.zeros((eeg_data.shape[0], ecog_data.shape[1]))\n",
    "#     new_eeg_data[ch] = dp.downsample_data(eeg_data[ch], ecog_data.shape[1])\n",
    "eeg_data = dp.downsample_data(eeg_data, ecog_data.shape[1])\n",
    "# Transpose data\n",
    "eeg_data = eeg_data.T     # (samples, channel)\n",
    "ecog_data = ecog_data.T   # (samples, channel)\n",
    "\n",
    "print(\"eeg_data.shape: \", eeg_data.shape)\n",
    "print(\"ecog_data.shape: \", ecog_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape:  (287310, 129)\n",
      "X_test shape:  (31924, 129)\n",
      "y_train shape:  (287310, 19)\n",
      "y_test shape:  (31924, 19)\n"
     ]
    }
   ],
   "source": [
    "# Split data into training, validation and test\n",
    "random_section = random.randint(0,9)\n",
    "X_train = np.vstack((ecog_data[:ecog_data.shape[0]*random_section//10,:], ecog_data[ecog_data.shape[0]*(random_section+1)//10:,:]))\n",
    "X_test = ecog_data[ecog_data.shape[0]*random_section//10:ecog_data.shape[0]*(random_section+1)//10,:]\n",
    "y_train = np.vstack((eeg_data[:eeg_data.shape[0]*random_section//10,:], eeg_data[eeg_data.shape[0]*(random_section+1)//10:,:]))\n",
    "y_test = eeg_data[eeg_data.shape[0]*random_section//10:eeg_data.shape[0]*(random_section+1)//10,:]\n",
    "print(\"X_train shape: \",X_train.shape)  # (287310, 129)\n",
    "print(\"X_test shape: \", X_test.shape)   # (31924, 129)\n",
    "print(\"y_train shape: \",y_train.shape)  # (287310, 19)\n",
    "print(\"y_test shape: \", y_test.shape)   # (31924, 19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "# Get cpu, gpu or mps device for training.\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    epoch_loss = 0.0\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        epoch_loss += loss.item() * pred.size(0)\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "    return epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(X_tensor, y_tensor, model, loss_fn):\n",
    "    # Make predictions on validation set\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_pred = model(X_tensor)\n",
    "        test_loss = loss_fn(y_pred, y_tensor).item()\n",
    "        corc = torch.corrcoef(torch.stack((y_pred, y_tensor)).T)\n",
    "\n",
    "        print(f\"Test Error: \\n Accuracy: {(100*corc):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "        return test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4-fold cross-validation\n",
    "k = 4\n",
    "loss_dict = {}\n",
    "val_losses = []\n",
    "for i in range(k):\n",
    "    X_train_new = np.vstack((X_train[:i*X_train.shape[0]//k,:],X_train[(i+1)*X_train.shape[0]//k:,:]))\n",
    "    X_val = X_train[i*X_train.shape[0]//k:(i+1)*X_train.shape[0]//k,:]\n",
    "    # print(\"X_train_new shape: \",X_train_new.shape) # (215482, 129)\n",
    "    # print(\"X_val shape: \",X_val.shape)             # (71827, 129)\n",
    "    y_train_new = np.vstack((y_train[:i*y_train.shape[0]//k,:],y_train[(i+1)*y_train.shape[0]//k:,:]))\n",
    "    y_val = y_train[i*y_train.shape[0]//k:(i+1)*y_train.shape[0]//k,:]\n",
    "    # print(\"y_train_new shape: \",y_train_new.shape) # (215482, 19)\n",
    "    # print(\"y_val shape: \",y_val.shape)             # (71827, 19)\n",
    "\n",
    "    # possible hyperparameters\n",
    "    low_bound = 0.5\n",
    "    high_bound = 45\n",
    "    sampling_rate = 1000\n",
    "\n",
    "    # bandwidth_butterworth_filter\n",
    "    X_train_filtered = dp.butter_bandpass_filter(X_train_new.T, low_bound, high_bound, sampling_rate)\n",
    "    X_val_filtered = dp.butter_bandpass_filter(X_val.T, low_bound, high_bound, sampling_rate)\n",
    "\n",
    "    # PCA whitening\n",
    "    X_train_w = dp.whitening(X_train_filtered.T)\n",
    "    X_val_w = dp.whitening(X_val_filtered.T)\n",
    "\n",
    "    # Convert numpy arrays to PyTorch tensors\n",
    "    X_tensor = torch.tensor(X_train_w, dtype=torch.float32)\n",
    "    y_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "\n",
    "    # Instantiate the model\n",
    "    input_size = X_tensor.size(1)   # 129\n",
    "    output_size = y_tensor.size(1)  # 19\n",
    "    model = LinearRegressionModel(input_size, output_size).to(device)\n",
    "\n",
    "    # Define loss function and optimizer\n",
    "    loss_fn = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001, betas=(0.9,0.99))\n",
    "\n",
    "    # Create DataLoader for batch processing\n",
    "    dataset = TensorDataset(X_tensor, y_tensor)\n",
    "    dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "    # Keep track of loss and error\n",
    "    loss_values = []\n",
    "\n",
    "    num_epochs = 100\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        epoch_loss = train(dataloader, model, loss_fn, optimizer, 100)\n",
    "\n",
    "        # Calculate average loss and mse for the epoch\n",
    "        epoch_loss /= len(dataloader.dataset)\n",
    "\n",
    "        # Append the loss values to the lists\n",
    "        loss_values.append(epoch_loss)\n",
    "\n",
    "        # Print progress\n",
    "        if (epoch+1) % 10 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}')\n",
    "\n",
    "    X_val_tensor = torch.tensor(X_val_w, dtype=torch.float32)\n",
    "    y_val_tensor = torch.tensor(y_val, dtype=torch.float32)\n",
    "\n",
    "    val_loss = test(X_val_tensor, y_val_tensor)\n",
    "    val_losses.append(val_loss)\n",
    "\n",
    "avg_val_loss = np.average(val_losses)\n",
    "print(\"avg_val_loss: \", avg_val_loss) # on such and such hyperparameter"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cse481f",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
